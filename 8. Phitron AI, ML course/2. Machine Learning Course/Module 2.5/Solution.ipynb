{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ffa53a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "04f3db81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71.875\n",
      "72.5\n",
      "0    50\n",
      "1    60\n",
      "2    65\n",
      "3    70\n",
      "4    75\n",
      "5    80\n",
      "6    85\n",
      "7    90\n",
      "dtype: int64\n",
      "173.125\n",
      "2.2380952380952377 1.4960264830861911\n"
     ]
    }
   ],
   "source": [
    "# M-1,P-A\n",
    "\n",
    "num=pd.Series([50, 60, 65, 70, 75, 80, 85, 90])\n",
    "\n",
    "print(num.mean())\n",
    "print(num.median())\n",
    "print(num.mode()) # unique item all\n",
    "\n",
    "num2=pd.Series([50, 60, 65, 70, 75, 80, 85, 900])\n",
    "print(num2.mean())\n",
    "\n",
    "temp=pd.Series([ 29, 31, 33, 33, 32, 31, 30 ])\n",
    "\n",
    "print(temp.var(),temp.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6f5753d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.225 3.45 3.825 0.6000000000000001\n",
      "2.325 4.7250000000000005\n",
      "1.5\n",
      "25.0\n"
     ]
    }
   ],
   "source": [
    "# M-1 P-B\n",
    "\n",
    "weight=pd.Series([2.5, 3.0, 3.2, 3.3, 3.4, 3.5, 3.6, 3.9, 4.0, 4.5])\n",
    "\n",
    "p25=weight.quantile(.25)\n",
    "p50=weight.quantile(.50)\n",
    "p75=weight.quantile(.75)\n",
    "iqr=p75-p25\n",
    "print(p25,p50,p75,iqr)\n",
    "\n",
    "min_out=p25-iqr*1.5\n",
    "max_out=p75+iqr*1.5\n",
    "print(min_out,max_out)\n",
    "\n",
    "\n",
    "m=50\n",
    "s=10\n",
    "x=65\n",
    "z=(x-m)/s\n",
    "print(z)\n",
    "\n",
    "z=-2.5\n",
    "x=z*s+m\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1b8f1b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4 0.5 0.2 0.4\n"
     ]
    }
   ],
   "source": [
    "# M-2 P-A\n",
    "\n",
    "PA=40/100\n",
    "PB=50/100\n",
    "PA_B=20/100\n",
    "\n",
    "PA_by_B=PA_B/PB\n",
    "print(PA,PB,PA_B,PA_by_B)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7602733c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4000000000000001\n",
      "0.009803921568627442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:22: SyntaxWarning: invalid escape sequence '\\c'\n",
      "<>:22: SyntaxWarning: invalid escape sequence '\\c'\n",
      "C:\\Users\\MD. ASAD-AL-ADIL\\AppData\\Local\\Temp\\ipykernel_3716\\358763075.py:22: SyntaxWarning: invalid escape sequence '\\c'\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nDiscussing the Base-Rate EffectThis result is the perfect illustration of the base-rate effect (or base-rate fallacy).\\nThe Result: Despite using a test that is 99% sensitive and 99% specific, \\na positive result means there is still less than a 1% chance you actually have the disease.\\nThe Intuition (The Fallacy): Our intuition is to focus on the test\\'s 99% accuracy ($P(+|D)$) \\nand assume a positive test means we are very likely sick. We ignore the \"base rate.\"The Reality (The Effect): \\nThe base rate ($P(D)$) is the most important factor here. \\nThe disease is extremely rare (1 in 10,000). \\nThe \"pool\" of healthy people (9,999 out of 10,000) is massive compared to the tiny \"pool\" of sick people.\\nLet\\'s visualize this with 1,000,000 people:Sick People: $1,000,000 \\\\cdot 0.0001 = \\\\mathbf{100 \\text{ people}}$Healthy People: $1,000,000 \\\\cdot 0.9999 = \\\\mathbf{999,900 \\text{ people}}$Now, \\nlet\\'s test all of them:True Positives: (From the 100 sick people) $100 \\\\cdot 0.99 = \\\\mathbf{99 \\text{ people}}$False Positives: \\n(From the 999,900 healthy people) $999,900 \\\\cdot 0.01 = \\\\mathbf{9,999 \\text{ people}}$In total, \\n$99 + 9,999 = 10,098$ people test positive.If you are one of those 10,098 people with a positive test, \\nwhat is the chance you are actually sick (i.e., a \"True Positive\")?$$P(D | +) = \\x0crac{\\text{True Positives}}{\\text{Total Positives}} = \\x0crac{99}{10,098} \\x07pprox 0.0098 \\n\\text{ (or 0.98\\\\%)}$$This confirms our calculation. The vast majority of positive tests ($\\x07pprox$99.02%) are false positives \\nsimply because the base rate of the disease is so incredibly low.'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# M-2 P-B\n",
    "\n",
    "PS=100/1000\n",
    "PF=(60+40)/1000\n",
    "PFbyS=40/100\n",
    "PSbyF=PFbyS*PS/PF\n",
    "print(PSbyF)\n",
    "\n",
    "\n",
    "PD=1/10000\n",
    "Pd=1-PD\n",
    "P_plusbyD=0.99\n",
    "P_negbyd=0.99\n",
    "P_plusbyd=1-P_plusbyD\n",
    "\n",
    "P_plus=P_plusbyD*PD+P_plusbyd*Pd\n",
    "\n",
    "P_Dbyplus=P_plusbyD*PD/P_plus\n",
    "print(P_Dbyplus)\n",
    "\n",
    "# base rate effect:\n",
    "\"\"\"\n",
    "Discussing the Base-Rate EffectThis result is the perfect illustration of the base-rate effect (or base-rate fallacy).\n",
    "The Result: Despite using a test that is 99% sensitive and 99% specific, \n",
    "a positive result means there is still less than a 1% chance you actually have the disease.\n",
    "The Intuition (The Fallacy): Our intuition is to focus on the test's 99% accuracy ($P(+|D)$) \n",
    "and assume a positive test means we are very likely sick. We ignore the \"base rate.\"The Reality (The Effect): \n",
    "The base rate ($P(D)$) is the most important factor here. \n",
    "The disease is extremely rare (1 in 10,000). \n",
    "The \"pool\" of healthy people (9,999 out of 10,000) is massive compared to the tiny \"pool\" of sick people.\n",
    "Let's visualize this with 1,000,000 people:Sick People: $1,000,000 \\cdot 0.0001 = \\mathbf{100 \\text{ people}}$Healthy People: $1,000,000 \\cdot 0.9999 = \\mathbf{999,900 \\text{ people}}$Now, \n",
    "let's test all of them:True Positives: (From the 100 sick people) $100 \\cdot 0.99 = \\mathbf{99 \\text{ people}}$False Positives: \n",
    "(From the 999,900 healthy people) $999,900 \\cdot 0.01 = \\mathbf{9,999 \\text{ people}}$In total, \n",
    "$99 + 9,999 = 10,098$ people test positive.If you are one of those 10,098 people with a positive test, \n",
    "what is the chance you are actually sick (i.e., a \"True Positive\")?$$P(D | +) = \\frac{\\text{True Positives}}{\\text{Total Positives}} = \\frac{99}{10,098} \\approx 0.0098 \n",
    "\\text{ (or 0.98\\%)}$$This confirms our calculation. The vast majority of positive tests ($\\approx$99.02%) are false positives \n",
    "simply because the base rate of the disease is so incredibly low.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d4b727b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "0.89\n",
      "0.6\n",
      "0.8\n",
      "0.6857142857142857\n",
      "0.9058823529411765\n",
      "0.9625\n",
      "0.12\n"
     ]
    }
   ],
   "source": [
    "# M-2 P-C\n",
    "\n",
    "result={\n",
    "    'TP':120,'FN':30,'FP':80,'TN':770\n",
    "}\n",
    "total=sum([j for i,j in result.items()])\n",
    "print(total)\n",
    "\n",
    "accuracy=(result['TP']+result['TN'])/total\n",
    "print(accuracy)\n",
    "\n",
    "precision=result['TP']/(result['TP']+result['FP'])\n",
    "print(precision)\n",
    "\n",
    "recall=result['TP']/(result['TP']+result['FN'])\n",
    "print(recall)\n",
    "\n",
    "F1=(2*precision*recall)/(precision+recall)\n",
    "print(F1)\n",
    "\n",
    "specificity=result['TN']/(result['TN']+result['FP'])\n",
    "print(specificity)\n",
    "\n",
    "NPV=result['TN']/(result['TN']+result['FN'])\n",
    "print(NPV)\n",
    "\n",
    "prevalence=result['TP']/total\n",
    "print(prevalence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb6125f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.98\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.98\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# Task-02\n",
    "\n",
    "result={\n",
    "    'TP':0,'FN':200,'FP':0,'TN':9800 # calculated from hand notebook\n",
    "}\n",
    "total=sum([j for i,j in result.items()])\n",
    "# print(total)\n",
    "\n",
    "accuracy=(result['TP']+result['TN'])/total\n",
    "print(accuracy)\n",
    "\n",
    "# precision=result['TP']/(result['TP']+result['FP'])\n",
    "# print(precision)\n",
    "\n",
    "recall=result['TP']/(result['TP']+result['FN'])\n",
    "print(recall)\n",
    "\n",
    "F1=(2*precision*recall)/(precision+recall)\n",
    "print(F1)\n",
    "\n",
    "specificity=result['TN']/(result['TN']+result['FP'])\n",
    "print(specificity)\n",
    "\n",
    "NPV=result['TN']/(result['TN']+result['FN'])\n",
    "print(NPV)\n",
    "\n",
    "prevalence=result['TP']/total\n",
    "print(prevalence)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Accuracy is misleading because it is completely dominated by the majority class (the negatives).\n",
    "\n",
    "The model achieved 98% accuracy by simply guessing \"negative\" every single time. \n",
    "This high score gives a false sense of a good model, but it comes entirely from correctly identifying the 9,800 negatives.\n",
    "\n",
    "The accuracy score completely hides the fact that the model failed at its primary objective: \n",
    "it found 0% of the positive samples. In most imbalanced problems \n",
    "(like fraud detection, medical diagnosis, or spam filtering), the minority class (the 2% \"positives\") \n",
    "is the one we actually care about finding.    \n",
    "\n",
    "\n",
    "Recall and F1-Score are far more appropriate for imbalanced data.\n",
    "\n",
    "Recall: This is the most telling metric in this case. It answers the question, \n",
    "\"Of all the actual positive samples, how many did the model find?\" \n",
    "Our model's Recall of 0% instantly reveals that it is completely useless for identifying positive cases.\n",
    "\n",
    "F1-Score: This metric is the harmonic mean of Precision and Recall. \n",
    "It provides a single score that balances both. \n",
    "The F1-Score will only be high if both precision and recall are reasonably high. \n",
    "Our model's F1-Score of 0% clearly and correctly states that the model has zero value for the positive class.\n",
    "\n",
    "Conclusion: For imbalanced data, you should always look at metrics that focus on the minority class \n",
    "(like Recall, Precision, and F1-Score) rather than the overall accuracy, \n",
    "which is inflated by the model's performance on the uninteresting majority class. \n",
    "   \n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
